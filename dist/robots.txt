# robots.txt for https://jkgadirli.dev/

# Allow all crawlers to access all content
User-agent: *
Allow: /

# Sitemap location
Sitemap: https://jkgadirli.dev/sitemap.xml

# Crawl delay (optional - uncomment if needed)
# Crawl-delay: 10

# Block specific directories if needed (examples - uncomment/modify as needed)
# Disallow: /private/
# Disallow: /admin/
# Disallow: /api/
# Disallow: /*.json$
# Disallow: /temp/

# Block access to specific file types (examples - uncomment if needed)
# Disallow: /*.pdf$
# Disallow: /*.zip$
# Disallow: /*.doc$

# Common technical directories to block (uncomment if they exist)
# Disallow: /node_modules/
# Disallow: /.git/
# Disallow: /.env
# Disallow: /config/
# Disallow: /_next/static/

# Block specific crawlers (examples - uncomment if needed)
# User-agent: GPTBot
# Disallow: /
#
# User-agent: CCBot
# Disallow: /
#
# User-agent: ChatGPT-User
# Disallow: /
#
# User-agent: Google-Extended
# Disallow: /

# Allow search engines to index images
User-agent: Googlebot-Image
Allow: /

# Allow search engines to index videos
User-agent: Googlebot-Video
Allow: /

# Allow Google Analytics tracking scripts
User-agent: *
Allow: /gtag/
Allow: /*.js$